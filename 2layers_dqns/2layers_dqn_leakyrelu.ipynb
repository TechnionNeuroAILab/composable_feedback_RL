{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# DQN Training — CartPole-v1\n",
    "Tracks per-step: **loss**, **Q values** (per action + per feature contribution), **reward**, **dominant feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    0 | Reward:   13.0 | Epsilon: 0.999 | w1 norm: 4.46\n",
      "Episode  100 | Reward:   13.0 | Epsilon: 0.904 | w1 norm: 6.98\n",
      "Episode  200 | Reward:   10.0 | Epsilon: 0.818 | w1 norm: 12.07\n",
      "Episode  300 | Reward:   39.0 | Epsilon: 0.740 | w1 norm: 15.92\n",
      "Episode  400 | Reward:  128.0 | Epsilon: 0.670 | w1 norm: 19.64\n",
      "Episode  500 | Reward:   72.0 | Epsilon: 0.606 | w1 norm: 23.63\n",
      "Episode  600 | Reward:  118.0 | Epsilon: 0.548 | w1 norm: 27.86\n",
      "Episode  700 | Reward:  363.0 | Epsilon: 0.496 | w1 norm: 32.44\n",
      "Episode  800 | Reward:   60.0 | Epsilon: 0.449 | w1 norm: 35.50\n",
      "Episode  900 | Reward:   49.0 | Epsilon: 0.406 | w1 norm: 39.25\n",
      "Episode 1000 | Reward:  229.0 | Epsilon: 0.367 | w1 norm: 42.33\n",
      "Episode 1100 | Reward:  204.0 | Epsilon: 0.332 | w1 norm: 43.92\n",
      "Episode 1200 | Reward:  500.0 | Epsilon: 0.301 | w1 norm: 44.72\n",
      "Episode 1300 | Reward:   51.0 | Epsilon: 0.272 | w1 norm: 46.17\n",
      "Episode 1400 | Reward:   39.0 | Epsilon: 0.246 | w1 norm: 48.82\n",
      "Episode 1500 | Reward:   99.0 | Epsilon: 0.223 | w1 norm: 49.24\n",
      "Episode 1600 | Reward:  122.0 | Epsilon: 0.202 | w1 norm: 49.59\n",
      "Episode 1700 | Reward:  395.0 | Epsilon: 0.182 | w1 norm: 51.20\n",
      "Episode 1800 | Reward:  215.0 | Epsilon: 0.165 | w1 norm: 52.20\n",
      "Episode 1900 | Reward:  174.0 | Epsilon: 0.149 | w1 norm: 52.06\n",
      "Episode 2000 | Reward:  114.0 | Epsilon: 0.135 | w1 norm: 52.12\n",
      "Episode 2100 | Reward:  299.0 | Epsilon: 0.122 | w1 norm: 52.56\n",
      "Episode 2200 | Reward:  500.0 | Epsilon: 0.111 | w1 norm: 51.81\n",
      "Episode 2300 | Reward:  133.0 | Epsilon: 0.100 | w1 norm: 50.68\n",
      "Episode 2400 | Reward:  165.0 | Epsilon: 0.091 | w1 norm: 51.53\n",
      "Episode 2500 | Reward:  500.0 | Epsilon: 0.082 | w1 norm: 50.90\n",
      "Episode 2600 | Reward:  500.0 | Epsilon: 0.074 | w1 norm: 51.23\n",
      "Episode 2700 | Reward:  500.0 | Epsilon: 0.067 | w1 norm: 51.98\n",
      "Episode 2800 | Reward:  233.0 | Epsilon: 0.061 | w1 norm: 53.04\n",
      "Episode 2900 | Reward:  112.0 | Epsilon: 0.055 | w1 norm: 52.88\n",
      "Episode 3000 | Reward:  120.0 | Epsilon: 0.050 | w1 norm: 52.97\n",
      "Episode 3100 | Reward:  148.0 | Epsilon: 0.050 | w1 norm: 53.42\n",
      "Episode 3200 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 53.75\n",
      "Episode 3300 | Reward:  171.0 | Epsilon: 0.050 | w1 norm: 53.49\n",
      "Episode 3400 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 54.77\n",
      "Episode 3500 | Reward:  370.0 | Epsilon: 0.050 | w1 norm: 54.64\n",
      "Episode 3600 | Reward:  145.0 | Epsilon: 0.050 | w1 norm: 55.18\n",
      "Episode 3700 | Reward:  194.0 | Epsilon: 0.050 | w1 norm: 54.75\n",
      "Episode 3800 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 54.17\n",
      "Episode 3900 | Reward:  105.0 | Epsilon: 0.050 | w1 norm: 51.95\n",
      "Episode 4000 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 53.31\n",
      "Episode 4100 | Reward:  432.0 | Epsilon: 0.050 | w1 norm: 53.44\n",
      "Episode 4200 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 54.15\n",
      "Episode 4300 | Reward:   55.0 | Epsilon: 0.050 | w1 norm: 54.22\n",
      "Episode 4400 | Reward:  151.0 | Epsilon: 0.050 | w1 norm: 54.68\n",
      "Episode 4500 | Reward:   52.0 | Epsilon: 0.050 | w1 norm: 54.19\n",
      "Episode 4600 | Reward:  123.0 | Epsilon: 0.050 | w1 norm: 54.86\n",
      "Episode 4700 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 54.83\n",
      "Episode 4800 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 55.56\n",
      "Episode 4900 | Reward:  107.0 | Epsilon: 0.050 | w1 norm: 56.00\n",
      "Episode 5000 | Reward:   94.0 | Epsilon: 0.050 | w1 norm: 56.18\n",
      "Episode 5100 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 56.58\n",
      "Episode 5200 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 57.81\n",
      "Episode 5300 | Reward:  208.0 | Epsilon: 0.050 | w1 norm: 58.88\n",
      "Episode 5400 | Reward:  340.0 | Epsilon: 0.050 | w1 norm: 60.69\n",
      "Episode 5500 | Reward:  207.0 | Epsilon: 0.050 | w1 norm: 59.67\n",
      "Episode 5600 | Reward:  119.0 | Epsilon: 0.050 | w1 norm: 59.72\n",
      "Episode 5700 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 60.17\n",
      "Episode 5800 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 59.34\n",
      "Episode 5900 | Reward:  479.0 | Epsilon: 0.050 | w1 norm: 59.26\n",
      "Episode 6000 | Reward:  298.0 | Epsilon: 0.050 | w1 norm: 59.52\n",
      "Episode 6100 | Reward:  111.0 | Epsilon: 0.050 | w1 norm: 60.05\n",
      "Episode 6200 | Reward:  213.0 | Epsilon: 0.050 | w1 norm: 59.80\n",
      "Episode 6300 | Reward:  260.0 | Epsilon: 0.050 | w1 norm: 60.68\n",
      "Episode 6400 | Reward:  207.0 | Epsilon: 0.050 | w1 norm: 59.99\n",
      "Episode 6500 | Reward:  122.0 | Epsilon: 0.050 | w1 norm: 58.21\n",
      "Episode 6600 | Reward:  118.0 | Epsilon: 0.050 | w1 norm: 58.83\n",
      "Episode 6700 | Reward:  128.0 | Epsilon: 0.050 | w1 norm: 58.62\n",
      "Episode 6800 | Reward:  144.0 | Epsilon: 0.050 | w1 norm: 57.81\n",
      "Episode 6900 | Reward:  151.0 | Epsilon: 0.050 | w1 norm: 58.44\n",
      "Episode 7000 | Reward:  162.0 | Epsilon: 0.050 | w1 norm: 58.82\n",
      "Episode 7100 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 59.00\n",
      "Episode 7200 | Reward:  245.0 | Epsilon: 0.050 | w1 norm: 60.01\n",
      "Episode 7300 | Reward:  135.0 | Epsilon: 0.050 | w1 norm: 61.31\n",
      "Episode 7400 | Reward:  408.0 | Epsilon: 0.050 | w1 norm: 61.98\n",
      "Episode 7500 | Reward:  150.0 | Epsilon: 0.050 | w1 norm: 62.84\n",
      "Episode 7600 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 63.51\n",
      "Episode 7700 | Reward:  115.0 | Epsilon: 0.050 | w1 norm: 67.12\n",
      "Episode 7800 | Reward:  100.0 | Epsilon: 0.050 | w1 norm: 68.68\n",
      "Episode 7900 | Reward:  103.0 | Epsilon: 0.050 | w1 norm: 67.19\n",
      "Episode 8000 | Reward:  114.0 | Epsilon: 0.050 | w1 norm: 65.85\n",
      "Episode 8100 | Reward:  237.0 | Epsilon: 0.050 | w1 norm: 67.13\n",
      "Episode 8200 | Reward:  130.0 | Epsilon: 0.050 | w1 norm: 67.66\n",
      "Episode 8300 | Reward:   34.0 | Epsilon: 0.050 | w1 norm: 68.16\n",
      "Episode 8400 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 69.08\n",
      "Episode 8500 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 69.69\n",
      "Episode 8600 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 70.22\n",
      "Episode 8700 | Reward:  162.0 | Epsilon: 0.050 | w1 norm: 69.07\n",
      "Episode 8800 | Reward:  135.0 | Epsilon: 0.050 | w1 norm: 68.51\n",
      "Episode 8900 | Reward:  207.0 | Epsilon: 0.050 | w1 norm: 68.87\n",
      "Episode 9000 | Reward:  115.0 | Epsilon: 0.050 | w1 norm: 68.35\n",
      "Episode 9100 | Reward:  104.0 | Epsilon: 0.050 | w1 norm: 67.77\n",
      "Episode 9200 | Reward:  206.0 | Epsilon: 0.050 | w1 norm: 67.48\n",
      "Episode 9300 | Reward:  313.0 | Epsilon: 0.050 | w1 norm: 67.67\n",
      "Episode 9400 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 67.55\n",
      "Episode 9500 | Reward:  132.0 | Epsilon: 0.050 | w1 norm: 66.82\n",
      "Episode 9600 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 66.90\n",
      "Episode 9700 | Reward:  222.0 | Epsilon: 0.050 | w1 norm: 67.75\n",
      "Episode 9800 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 68.03\n",
      "Episode 9900 | Reward:  154.0 | Epsilon: 0.050 | w1 norm: 68.57\n",
      "Episode 10000 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 69.32\n",
      "Episode 10100 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 70.06\n",
      "Episode 10200 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 70.83\n",
      "Episode 10300 | Reward:  450.0 | Epsilon: 0.050 | w1 norm: 72.09\n",
      "Episode 10400 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 72.88\n",
      "Episode 10500 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 73.74\n",
      "Episode 10600 | Reward:  180.0 | Epsilon: 0.050 | w1 norm: 74.11\n",
      "Episode 10700 | Reward:  111.0 | Epsilon: 0.050 | w1 norm: 73.88\n",
      "Episode 10800 | Reward:  326.0 | Epsilon: 0.050 | w1 norm: 73.61\n",
      "Episode 10900 | Reward:  135.0 | Epsilon: 0.050 | w1 norm: 73.24\n",
      "Episode 11000 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 74.16\n",
      "Episode 11100 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 73.66\n",
      "Episode 11200 | Reward:  109.0 | Epsilon: 0.050 | w1 norm: 73.90\n",
      "Episode 11300 | Reward:  398.0 | Epsilon: 0.050 | w1 norm: 74.38\n",
      "Episode 11400 | Reward:   89.0 | Epsilon: 0.050 | w1 norm: 74.31\n",
      "Episode 11500 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 75.00\n",
      "Episode 11600 | Reward:  111.0 | Epsilon: 0.050 | w1 norm: 76.04\n",
      "Episode 11700 | Reward:  132.0 | Epsilon: 0.050 | w1 norm: 76.28\n",
      "Episode 11800 | Reward:  103.0 | Epsilon: 0.050 | w1 norm: 76.33\n",
      "Episode 11900 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 76.21\n",
      "Episode 12000 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 76.74\n",
      "Episode 12100 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 77.33\n",
      "Episode 12200 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 75.92\n",
      "Episode 12300 | Reward:  301.0 | Epsilon: 0.050 | w1 norm: 75.61\n",
      "Episode 12400 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 75.08\n",
      "Episode 12500 | Reward:  309.0 | Epsilon: 0.050 | w1 norm: 75.07\n",
      "Episode 12600 | Reward:  108.0 | Epsilon: 0.050 | w1 norm: 75.07\n",
      "Episode 12700 | Reward:  147.0 | Epsilon: 0.050 | w1 norm: 74.21\n",
      "Episode 12800 | Reward:  179.0 | Epsilon: 0.050 | w1 norm: 73.47\n",
      "Episode 12900 | Reward:   18.0 | Epsilon: 0.050 | w1 norm: 74.13\n",
      "Episode 13000 | Reward:   97.0 | Epsilon: 0.050 | w1 norm: 75.30\n",
      "Episode 13100 | Reward:   16.0 | Epsilon: 0.050 | w1 norm: 75.30\n",
      "Episode 13200 | Reward:  174.0 | Epsilon: 0.050 | w1 norm: 76.12\n",
      "Episode 13300 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 75.85\n",
      "Episode 13400 | Reward:  146.0 | Epsilon: 0.050 | w1 norm: 76.05\n",
      "Episode 13500 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 76.19\n",
      "Episode 13600 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 76.66\n",
      "Episode 13700 | Reward:  424.0 | Epsilon: 0.050 | w1 norm: 77.33\n",
      "Episode 13800 | Reward:  125.0 | Epsilon: 0.050 | w1 norm: 77.27\n",
      "Episode 13900 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 78.64\n",
      "Episode 14000 | Reward:  196.0 | Epsilon: 0.050 | w1 norm: 79.62\n",
      "Episode 14100 | Reward:  156.0 | Epsilon: 0.050 | w1 norm: 79.31\n",
      "Episode 14200 | Reward:  224.0 | Epsilon: 0.050 | w1 norm: 79.33\n",
      "Episode 14300 | Reward:  331.0 | Epsilon: 0.050 | w1 norm: 79.88\n",
      "Episode 14400 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 79.93\n",
      "Episode 14500 | Reward:  267.0 | Epsilon: 0.050 | w1 norm: 79.40\n",
      "Episode 14600 | Reward:   99.0 | Epsilon: 0.050 | w1 norm: 79.36\n",
      "Episode 14700 | Reward:  110.0 | Epsilon: 0.050 | w1 norm: 79.32\n",
      "Episode 14800 | Reward:  166.0 | Epsilon: 0.050 | w1 norm: 79.14\n",
      "Episode 14900 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 79.12\n",
      "Episode 15000 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 78.92\n",
      "Episode 15100 | Reward:  141.0 | Epsilon: 0.050 | w1 norm: 78.46\n",
      "Episode 15200 | Reward:  130.0 | Epsilon: 0.050 | w1 norm: 77.92\n",
      "Episode 15300 | Reward:  162.0 | Epsilon: 0.050 | w1 norm: 78.44\n",
      "Episode 15400 | Reward:  110.0 | Epsilon: 0.050 | w1 norm: 78.88\n",
      "Episode 15500 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 78.64\n",
      "Episode 15600 | Reward:  129.0 | Epsilon: 0.050 | w1 norm: 78.32\n",
      "Episode 15700 | Reward:   97.0 | Epsilon: 0.050 | w1 norm: 77.32\n",
      "Episode 15800 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 77.12\n",
      "Episode 15900 | Reward:  148.0 | Epsilon: 0.050 | w1 norm: 76.45\n",
      "Episode 16000 | Reward:  307.0 | Epsilon: 0.050 | w1 norm: 76.55\n",
      "Episode 16100 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 76.57\n",
      "Episode 16200 | Reward:  132.0 | Epsilon: 0.050 | w1 norm: 76.99\n",
      "Episode 16300 | Reward:  107.0 | Epsilon: 0.050 | w1 norm: 78.07\n",
      "Episode 16400 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 77.24\n",
      "Episode 16500 | Reward:   22.0 | Epsilon: 0.050 | w1 norm: 76.92\n",
      "Episode 16600 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 77.87\n",
      "Episode 16700 | Reward:  116.0 | Epsilon: 0.050 | w1 norm: 77.84\n",
      "Episode 16800 | Reward:  249.0 | Epsilon: 0.050 | w1 norm: 76.52\n",
      "Episode 16900 | Reward:  225.0 | Epsilon: 0.050 | w1 norm: 75.26\n",
      "Episode 17000 | Reward:  205.0 | Epsilon: 0.050 | w1 norm: 74.36\n",
      "Episode 17100 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 73.72\n",
      "Episode 17200 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 73.75\n",
      "Episode 17300 | Reward:  215.0 | Epsilon: 0.050 | w1 norm: 74.82\n",
      "Episode 17400 | Reward:  256.0 | Epsilon: 0.050 | w1 norm: 75.12\n",
      "Episode 17500 | Reward:  163.0 | Epsilon: 0.050 | w1 norm: 74.78\n",
      "Episode 17600 | Reward:  152.0 | Epsilon: 0.050 | w1 norm: 75.35\n",
      "Episode 17700 | Reward:  121.0 | Epsilon: 0.050 | w1 norm: 74.92\n",
      "Episode 17800 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 75.44\n",
      "Episode 17900 | Reward:  112.0 | Epsilon: 0.050 | w1 norm: 76.45\n",
      "Episode 18000 | Reward:  120.0 | Epsilon: 0.050 | w1 norm: 76.16\n",
      "Episode 18100 | Reward:  267.0 | Epsilon: 0.050 | w1 norm: 76.49\n",
      "Episode 18200 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 77.20\n",
      "Episode 18300 | Reward:  500.0 | Epsilon: 0.050 | w1 norm: 77.28\n",
      "Episode 18400 | Reward:  344.0 | Epsilon: 0.050 | w1 norm: 75.87\n",
      "Episode 18500 | Reward:  369.0 | Epsilon: 0.050 | w1 norm: 75.85\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "FEATURE_NAMES  = [\"cart_pos\", \"cart_vel\", \"pole_angle\", \"pole_ang_vel\"]\n",
    "FEATURE_LABELS = [\"Cart Position\", \"Cart Velocity\", \"Pole Angle\", \"Pole Ang. Vel\"]\n",
    "\n",
    "# ── Network ───────────────────────────────────────────────────────────────────\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(np.array(env.observation_space.shape).prod(), hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.leaky_relu(self.layer1(x))\n",
    "        return self.layer2(y)\n",
    "\n",
    "    @property\n",
    "    def w1(self): return self.layer1.weight\n",
    "    @property\n",
    "    def w2(self): return self.layer2.weight\n",
    "\n",
    "\n",
    "# ── Feature contribution helper ───────────────────────────────────────────────\n",
    "def get_feature_contributions(q_network, state):\n",
    "    \"\"\"Returns q_values (n_actions,), contributions (n_actions, n_feats), dominant_feat (n_actions,).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        x             = torch.FloatTensor(state)\n",
    "        h             = F.leaky_relu(q_network.layer1(x))           # post-activation hidden\n",
    "        contributions = (q_network.w2 * h.unsqueeze(0)).numpy()     # (n_actions, hidden_dim)\n",
    "        q_values      = q_network.layer2(h).numpy()                 # (n_actions,)\n",
    "        dominant_feat = np.argmax(np.abs(contributions), axis=1)\n",
    "    return q_values, contributions, dominant_feat\n",
    "\n",
    "\n",
    "# ── Hyperparameters ───────────────────────────────────────────────────────────\n",
    "LR            = 1e-3\n",
    "GAMMA         = 0.99\n",
    "BATCH_SIZE    = 64\n",
    "BUFFER_SIZE   = 10000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END   = 0.05\n",
    "EPSILON_DECAY = 0.999\n",
    "TARGET_UPDATE = 500      # steps\n",
    "N_EPISODES    = 20000\n",
    "\n",
    "# ── Setup ─────────────────────────────────────────────────────────────────────\n",
    "env            = gym.make(\"CartPole-v1\")\n",
    "q_network      = QNetwork(env)\n",
    "target_network = QNetwork(env)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "optimizer      = optim.Adam(q_network.parameters(), lr=LR)\n",
    "memory         = deque(maxlen=BUFFER_SIZE)\n",
    "epsilon        = EPSILON_START\n",
    "\n",
    "step_log    = []   # one dict per env step\n",
    "episode_log = []   # one dict per episode\n",
    "global_step = 0\n",
    "\n",
    "# ── Training loop ─────────────────────────────────────────────────────────────\n",
    "for episode in range(N_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        q_values, contributions, dominant_feat = get_feature_contributions(q_network, state)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = int(np.argmax(q_values))\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        total_reward += reward\n",
    "\n",
    "        # Training step\n",
    "        loss_val = None\n",
    "        if len(memory) > BATCH_SIZE:\n",
    "            s_b, a_b, r_b, ns_b, d_b = zip(*random.sample(memory, BATCH_SIZE))\n",
    "            s_b  = torch.FloatTensor(np.array(s_b))\n",
    "            a_b  = torch.LongTensor(a_b).unsqueeze(1)\n",
    "            r_b  = torch.FloatTensor(r_b)\n",
    "            ns_b = torch.FloatTensor(np.array(ns_b))\n",
    "            d_b  = torch.FloatTensor(d_b)\n",
    "\n",
    "            current_q  = q_network(s_b).gather(1, a_b).squeeze()\n",
    "            with torch.no_grad():\n",
    "                target_q = r_b + (1 - d_b) * GAMMA * target_network(ns_b).max(1)[0]\n",
    "\n",
    "            loss = F.mse_loss(current_q, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_val = loss.item()\n",
    "\n",
    "        # Sync target network every TARGET_UPDATE steps\n",
    "        if global_step % TARGET_UPDATE == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # Record step\n",
    "        # contributions here is (n_actions, hidden_dim) — not per input feature\n",
    "        # dominant_feat indexes into hidden units, not FEATURE_NAMES, so we skip that column\n",
    "        row = {\n",
    "            \"episode\":      episode,\n",
    "            \"global_step\":  global_step,\n",
    "            \"action\":       action,\n",
    "            \"epsilon\":      epsilon,\n",
    "            \"loss\":         loss_val,\n",
    "            \"q_left\":       float(q_values[0]),\n",
    "            \"q_right\":      float(q_values[1]),\n",
    "            \"q_chosen\":     float(q_values[action]),\n",
    "        }\n",
    "\n",
    "        step_log.append(row)\n",
    "        state = next_state\n",
    "        global_step += 1\n",
    "\n",
    "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode:4d} | Reward: {total_reward:6.1f} | Epsilon: {epsilon:.3f} | w1 norm: {q_network.w1.norm().item():.2f}\")\n",
    "\n",
    "    episode_log.append({\"episode\": episode, \"reward\": total_reward})\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Build DataFrames\n",
    "df_steps = pd.DataFrame(step_log)\n",
    "df_ep    = pd.DataFrame(episode_log)\n",
    "\n",
    "# Episode-level aggregates\n",
    "df_ep_agg = df_steps.groupby(\"episode\").agg(\n",
    "    mean_loss=(\"loss\", \"mean\"),\n",
    "    mean_q=(\"q_chosen\", \"mean\"),\n",
    ").reset_index()\n",
    "df_ep_agg = df_ep_agg.merge(df_ep, on=\"episode\")\n",
    "\n",
    "print(f\"\\nCollected {len(df_steps):,} steps across {N_EPISODES} episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-md",
   "metadata": {},
   "source": [
    "## Training Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# ── Style ─────────────────────────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\":  \"#0a0e1a\",\n",
    "    \"axes.facecolor\":    \"#0f1525\",\n",
    "    \"axes.edgecolor\":    \"#1c2540\",\n",
    "    \"axes.labelcolor\":   \"#ffffff\",  # Changed to white\n",
    "    \"axes.titlecolor\":   \"#ffffff\",  # Changed to white\n",
    "    \"xtick.color\":       \"#ffffff\",  # Changed to white\n",
    "    \"ytick.color\":       \"#ffffff\",  # Changed to white\n",
    "    \"grid.color\":        \"#1c2540\",\n",
    "    \"grid.linewidth\":    0.6,\n",
    "    \"text.color\":        \"#ffffff\",  # Changed to white\n",
    "    \"font.family\":       \"monospace\",\n",
    "    \"axes.spines.top\":   False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "COLORS = {\n",
    "    \"reward\":   \"#43e97b\",\n",
    "    \"loss\":     \"#ff5f6d\",\n",
    "    \"q\":        \"#f5c518\",\n",
    "    \"features\": [\"#00e5ff\", \"#f5c518\", \"#ff5f6d\", \"#43e97b\"],\n",
    "    \"left\":     \"#a78bfa\",\n",
    "    \"right\":    \"#f97316\",\n",
    "    \"fill\":     0.15,\n",
    "}\n",
    "\n",
    "def smooth(s, w=30):\n",
    "    \"\"\"Pandas rolling mean.\"\"\"\n",
    "    return s.rolling(w, min_periods=1).mean()\n",
    "\n",
    "def styled_ax(ax, title, xlabel=\"Episode\", ylabel=\"\"):\n",
    "    ax.set_title(title, fontsize=10, fontweight=\"bold\", pad=8, color=\"#e2e8f0\")\n",
    "    ax.set_xlabel(xlabel, fontsize=8)\n",
    "    ax.set_ylabel(ylabel, fontsize=8)\n",
    "    ax.grid(True, axis=\"y\", alpha=0.4)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(5, integer=True))\n",
    "\n",
    "def fill_between(ax, x, y, color, alpha=0.12):\n",
    "    ax.fill_between(x, y, alpha=alpha, color=color)\n",
    "\n",
    "# ── Layout ────────────────────────────────────────────────────────────────────\n",
    "#  Row 0: Reward  |  Loss  |  Q-value\n",
    "#  Row 1: Q per action (left vs right)  |  Dominant feature (bar)\n",
    "#  Row 2: Feature contributions [4 subplots]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 14), facecolor=\"#0a0e1a\")\n",
    "fig.suptitle(\"DQN Training Dashboard — CartPole-v1\", fontsize=14,\n",
    "             fontweight=\"bold\", color=\"#00e5ff\", y=0.98)\n",
    "\n",
    "gs = gridspec.GridSpec(3, 4, figure=fig, hspace=0.52, wspace=0.38,\n",
    "                       left=0.06, right=0.97, top=0.93, bottom=0.06)\n",
    "\n",
    "ep  = df_ep_agg[\"episode\"]\n",
    "\n",
    "# ── 1. Reward ─────────────────────────────────────────────────────────────────\n",
    "ax_rew = fig.add_subplot(gs[0, 0])\n",
    "raw = df_ep_agg[\"reward\"]\n",
    "smo = smooth(raw)\n",
    "ax_rew.plot(ep, raw, color=COLORS[\"reward\"], alpha=0.25, linewidth=0.6)\n",
    "ax_rew.plot(ep, smo, color=COLORS[\"reward\"], linewidth=1.8, label=\"Smoothed\")\n",
    "fill_between(ax_rew, ep, smo, COLORS[\"reward\"])\n",
    "styled_ax(ax_rew, \"Episode Reward\", ylabel=\"Steps survived\")\n",
    "\n",
    "# ── 2. Loss ───────────────────────────────────────────────────────────────────\n",
    "ax_loss = fig.add_subplot(gs[0, 1])\n",
    "loss_s = df_ep_agg[\"mean_loss\"].dropna()\n",
    "ep_l   = df_ep_agg.loc[loss_s.index, \"episode\"]\n",
    "smo_l  = smooth(loss_s)\n",
    "ax_loss.plot(ep_l, loss_s, color=COLORS[\"loss\"], alpha=0.2, linewidth=0.6)\n",
    "ax_loss.plot(ep_l, smo_l,  color=COLORS[\"loss\"], linewidth=1.8)\n",
    "fill_between(ax_loss, ep_l, smo_l, COLORS[\"loss\"])\n",
    "styled_ax(ax_loss, \"Training Loss (MSE)\", ylabel=\"Loss\")\n",
    "\n",
    "# ── 3. Mean Q value ───────────────────────────────────────────────────────────\n",
    "ax_q = fig.add_subplot(gs[0, 2])\n",
    "q_s  = smooth(df_ep_agg[\"mean_q\"])\n",
    "ax_q.plot(ep, df_ep_agg[\"mean_q\"], color=COLORS[\"q\"], alpha=0.2, linewidth=0.6)\n",
    "ax_q.plot(ep, q_s, color=COLORS[\"q\"], linewidth=1.8)\n",
    "fill_between(ax_q, ep, q_s, COLORS[\"q\"])\n",
    "styled_ax(ax_q, \"Mean Q Value (chosen action)\", ylabel=\"Q\")\n",
    "\n",
    "# ── 4. Dominant feature pie/bar ───────────────────────────────────────────────\n",
    "ax_dom = fig.add_subplot(gs[0, 3])\n",
    "dom_counts = df_steps[\"dominant_feat\"].value_counts().reindex(FEATURE_NAMES, fill_value=0)\n",
    "bars = ax_dom.barh(\n",
    "    FEATURE_LABELS, dom_counts.values,\n",
    "    color=COLORS[\"features\"], edgecolor=\"none\", height=0.55\n",
    ")\n",
    "for bar, val in zip(bars, dom_counts.values):\n",
    "    ax_dom.text(bar.get_width() + max(dom_counts) * 0.01, bar.get_y() + bar.get_height() / 2,\n",
    "                f\"{val:,}\", va=\"center\", ha=\"left\", fontsize=8, color=\"#718096\")\n",
    "styled_ax(ax_dom, \"Dominant Feature (steps)\", xlabel=\"Step count\", ylabel=\"\")\n",
    "ax_dom.invert_yaxis()\n",
    "\n",
    "# ── 5. Q left vs Q right ──────────────────────────────────────────────────────\n",
    "ax_ql = fig.add_subplot(gs[1, :2])\n",
    "q_left_ep  = df_steps.groupby(\"episode\")[\"q_left\"].mean()\n",
    "q_right_ep = df_steps.groupby(\"episode\")[\"q_right\"].mean()\n",
    "ax_ql.plot(q_left_ep.index,  smooth(q_left_ep),  color=COLORS[\"left\"],  linewidth=1.6, label=\"Q Left  (action 0)\")\n",
    "ax_ql.plot(q_right_ep.index, smooth(q_right_ep), color=COLORS[\"right\"], linewidth=1.6, label=\"Q Right (action 1)\")\n",
    "fill_between(ax_ql, q_left_ep.index,  smooth(q_left_ep),  COLORS[\"left\"])\n",
    "fill_between(ax_ql, q_right_ep.index, smooth(q_right_ep), COLORS[\"right\"])\n",
    "styled_ax(ax_ql, \"Q Value per Action over Episodes\", ylabel=\"Mean Q\")\n",
    "ax_ql.legend(fontsize=8, framealpha=0.15)\n",
    "\n",
    "# ── 6. All feature contributions on one plot ──────────────────────────────────\n",
    "ax_fc = fig.add_subplot(gs[1, 2:])\n",
    "for fi, fname in enumerate(FEATURE_NAMES):\n",
    "    col = f\"mean_contrib_{fname}\"\n",
    "    s   = smooth(df_ep_agg[col])\n",
    "    ax_fc.plot(ep, s, color=COLORS[\"features\"][fi], linewidth=1.4, label=FEATURE_LABELS[fi])\n",
    "styled_ax(ax_fc, \"Mean |Feature Contribution| to Chosen Action\", ylabel=\"|Contribution|\")\n",
    "ax_fc.legend(fontsize=8, framealpha=0.15, ncol=2)\n",
    "\n",
    "# ── 7. Individual feature contribution plots ──────────────────────────────────\n",
    "for fi, (fname, flabel) in enumerate(zip(FEATURE_NAMES, FEATURE_LABELS)):\n",
    "    ax = fig.add_subplot(gs[2, fi])\n",
    "    col = f\"mean_contrib_{fname}\"\n",
    "    raw_f = df_ep_agg[col]\n",
    "    smo_f = smooth(raw_f)\n",
    "    c = COLORS[\"features\"][fi]\n",
    "    ax.plot(ep, raw_f, color=c, alpha=0.2, linewidth=0.5)\n",
    "    ax.plot(ep, smo_f, color=c, linewidth=1.6)\n",
    "    ax.axhline(0, color=\"#1c2540\", linewidth=0.8, linestyle=\"--\")\n",
    "    fill_between(ax, ep, smo_f.clip(lower=0), c, alpha=0.18)\n",
    "    fill_between(ax, ep, smo_f.clip(upper=0), c, alpha=0.18)\n",
    "    styled_ax(ax, flabel, ylabel=\"Contribution\")\n",
    "\n",
    "plt.savefig(\"dqn_dashboard_leakyrelu.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"#0a0e1a\")\n",
    "plt.show()\n",
    "print(\"Saved dqn_dashboard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "## Numeric Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Training Summary ===\")\n",
    "print(f\"  Max reward:          {df_ep['reward'].max():.0f}\")\n",
    "print(f\"  Final 100-ep mean:   {df_ep['reward'].tail(100).mean():.1f}\")\n",
    "print(f\"  Final mean loss:     {df_ep_agg['mean_loss'].dropna().tail(100).mean():.4f}\")\n",
    "print(f\"  Final mean Q:        {df_ep_agg['mean_q'].tail(100).mean():.3f}\")\n",
    "\n",
    "print(\"\\n=== Dominant Feature Distribution ===\")\n",
    "dom = df_steps[\"dominant_feat\"].value_counts()\n",
    "for f, c in dom.items():\n",
    "    print(f\"  {f:20s}: {c:6,}  ({100*c/len(df_steps):.1f}%)\")\n",
    "\n",
    "print(\"\\n=== Mean |Contribution| per Feature (last 500 eps) ===\")\n",
    "last = df_ep_agg.tail(500)\n",
    "for fname, flabel in zip(FEATURE_NAMES, FEATURE_LABELS):\n",
    "    print(f\"  {flabel:20s}: {last[f'mean_contrib_{fname}'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
